{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tyri0n11/Fraud-Detection-Kaggle/blob/main/Fraud_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1ICCThpp5qEd"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Fraud Detection\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORNaA4sS3LvS",
        "outputId": "48fc67f3-c0e1-4275-be3d-bb4cad4d3337"
      },
      "outputs": [
        {
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/Final Transaction.csv. SQLSTATE: 42K03",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3998492202.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final Transaction.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_remote_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/Final Transaction.csv. SQLSTATE: 42K03"
          ]
        }
      ],
      "source": [
        "df = spark.read.csv(\"Final Transaction.csv\", header=True, inferSchema=True)\n",
        "df.show(5)\n",
        "df.dropna()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CStdHgRD54vg",
        "outputId": "d100407f-5172-456a-fc65-1a7ee9572410"
      },
      "outputs": [],
      "source": [
        "df_clean = df.drop('_c0','TRANSACTION_ID', 'TX_FRAUD_SCENARIO')\n",
        "df_clean.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hvx27D09-b88"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_clean = df_clean.filter(col(\"TX_FRAUD\").isNotNull())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D3LqKGl8YPH",
        "outputId": "6aaa738a-bb8f-4084-de32-f3c075d11a9b"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr, hour, dayofweek\n",
        "\n",
        "df_clean = df_clean.withColumn(\n",
        "    \"TX_DATETIME\",\n",
        "    expr(\"try_to_timestamp(TX_DATETIME, 'yyyy-MM-dd HH:mm:ss')\")\n",
        ")\n",
        "\n",
        "df_clean = df_clean.withColumn(\"TX_HOUR\", hour(\"TX_DATETIME\")) \\\n",
        "                   .withColumn(\"TX_DAYOFWEEK\", dayofweek(\"TX_DATETIME\")) \\\n",
        "                   .drop(\"TX_DATETIME\")\n",
        "\n",
        "\n",
        "\n",
        "df_clean.show(5)\n",
        "df_clean.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Exploration Chi Tiết\n",
        "\n",
        "Phân tích dataset để hiểu rõ hơn về cấu trúc, phân phối và các đặc điểm của dữ liệu.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 1. Thống kê mô tả (Descriptive Statistics)\n",
        "# ============================================\n",
        "print(\"=\" * 60)\n",
        "print(\"THỐNG KÊ MÔ TẢ CHO CÁC FEATURES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Thống kê cho numerical features\n",
        "numerical_cols = [\"TX_AMOUNT\", \"TX_TIME_SECONDS\", \"TX_TIME_DAYS\", \"TX_HOUR\", \"TX_DAYOFWEEK\"]\n",
        "df_clean.select(numerical_cols).describe().show()\n",
        "\n",
        "# Thống kê cho categorical features\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"THỐNG KÊ CHO CATEGORICAL FEATURES\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nSố lượng unique customers:\", df_clean.select(\"CUSTOMER_ID\").distinct().count())\n",
        "print(\"Số lượng unique terminals:\", df_clean.select(\"TERMINAL_ID\").distinct().count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 2. Phân tích Missing Values\n",
        "# ============================================\n",
        "print(\"=\" * 60)\n",
        "print(\"PHÂN TÍCH MISSING VALUES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from pyspark.sql.functions import col, sum as spark_sum, count\n",
        "\n",
        "# Đếm missing values cho từng cột\n",
        "total_rows = df_clean.count()\n",
        "print(f\"\\nTổng số rows: {total_rows:,}\")\n",
        "\n",
        "missing_stats = []\n",
        "for col_name in df_clean.columns:\n",
        "    missing_count = df_clean.filter(col(col_name).isNull()).count()\n",
        "    missing_pct = (missing_count / total_rows) * 100 if total_rows > 0 else 0\n",
        "    missing_stats.append((col_name, missing_count, missing_pct))\n",
        "    if missing_count > 0:\n",
        "        print(f\"{col_name}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
        "\n",
        "if all(missing_count == 0 for _, missing_count, _ in missing_stats):\n",
        "    print(\"\\n✓ Không có missing values trong dataset!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 3. Phân tích Class Distribution\n",
        "# ============================================\n",
        "print(\"=\" * 60)\n",
        "print(\"PHÂN TÍCH CLASS DISTRIBUTION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from pyspark.sql.functions import count as spark_count\n",
        "\n",
        "class_dist = df_clean.groupBy(\"TX_FRAUD\").agg(\n",
        "    spark_count(\"*\").alias(\"count\")\n",
        ").orderBy(\"TX_FRAUD\")\n",
        "\n",
        "class_dist.show()\n",
        "\n",
        "# Tính tỷ lệ\n",
        "total = df_clean.count()\n",
        "fraud_count = df_clean.filter(col(\"TX_FRAUD\") == 1).count()\n",
        "normal_count = df_clean.filter(col(\"TX_FRAUD\") == 0).count()\n",
        "\n",
        "print(f\"\\nTổng số giao dịch: {total:,}\")\n",
        "print(f\"Giao dịch bình thường (0): {normal_count:,} ({normal_count/total*100:.2f}%)\")\n",
        "print(f\"Giao dịch gian lận (1): {fraud_count:,} ({fraud_count/total*100:.2f}%)\")\n",
        "print(f\"\\nTỷ lệ imbalance: {normal_count/fraud_count:.2f}:1 (Normal:Fraud)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 4. Phân tích Correlation (sử dụng Pandas cho visualization)\n",
        "# ============================================\n",
        "print(\"=\" * 60)\n",
        "print(\"PHÂN TÍCH CORRELATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Convert sang Pandas để tính correlation\n",
        "pdf_for_corr = df_clean.select(\n",
        "    \"TX_AMOUNT\", \"TX_TIME_SECONDS\", \"TX_TIME_DAYS\", \n",
        "    \"TX_HOUR\", \"TX_DAYOFWEEK\", \"TX_FRAUD\"\n",
        ").toPandas()\n",
        "\n",
        "correlation_matrix = pdf_for_corr.corr()\n",
        "\n",
        "# Vẽ correlation heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm', \n",
        "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title(\"Correlation Matrix của các Features\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCorrelation với TX_FRAUD:\")\n",
        "corr_with_fraud = correlation_matrix['TX_FRAUD'].sort_values(ascending=False)\n",
        "print(corr_with_fraud)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 5. Phân tích Distribution của Features theo Fraud Class\n",
        "# ============================================\n",
        "print(\"=\" * 60)\n",
        "print(\"PHÂN TÍCH DISTRIBUTION CỦA TX_AMOUNT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# So sánh distribution của TX_AMOUNT giữa fraud và non-fraud\n",
        "pdf_amount = df_clean.select(\"TX_AMOUNT\", \"TX_FRAUD\").toPandas()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Histogram\n",
        "fraud_amounts = pdf_amount[pdf_amount['TX_FRAUD'] == 1]['TX_AMOUNT']\n",
        "normal_amounts = pdf_amount[pdf_amount['TX_FRAUD'] == 0]['TX_AMOUNT']\n",
        "\n",
        "axes[0].hist(normal_amounts, bins=50, alpha=0.7, label='Normal', color='blue', density=True)\n",
        "axes[0].hist(fraud_amounts, bins=50, alpha=0.7, label='Fraud', color='red', density=True)\n",
        "axes[0].set_xlabel('TX_AMOUNT')\n",
        "axes[0].set_ylabel('Density')\n",
        "axes[0].set_title('Distribution of TX_AMOUNT by Class')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Box plot\n",
        "data_for_box = [normal_amounts, fraud_amounts]\n",
        "axes[1].boxplot(data_for_box, labels=['Normal', 'Fraud'])\n",
        "axes[1].set_ylabel('TX_AMOUNT')\n",
        "axes[1].set_title('Box Plot of TX_AMOUNT by Class')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Thống kê\n",
        "print(\"\\nThống kê TX_AMOUNT theo class:\")\n",
        "print(f\"Normal - Mean: {normal_amounts.mean():.2f}, Median: {normal_amounts.median():.2f}, Std: {normal_amounts.std():.2f}\")\n",
        "print(f\"Fraud  - Mean: {fraud_amounts.mean():.2f}, Median: {fraud_amounts.median():.2f}, Std: {fraud_amounts.std():.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering Nâng Cao\n",
        "\n",
        "Tạo các features bổ sung từ dữ liệu giao dịch để cải thiện khả năng dự đoán của model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Feature Engineering: Customer-level Statistics\n",
        "# ============================================\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import count as spark_count, avg, sum as spark_sum, stddev, max as spark_max, min as spark_min\n",
        "\n",
        "# Window function để tính statistics theo customer\n",
        "customer_window = Window.partitionBy(\"CUSTOMER_ID\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "# Tính các features theo customer\n",
        "df_with_customer_features = df_clean.withColumn(\n",
        "    \"CUSTOMER_TX_COUNT\", \n",
        "    spark_count(\"*\").over(customer_window)\n",
        ").withColumn(\n",
        "    \"CUSTOMER_AVG_AMOUNT\",\n",
        "    avg(\"TX_AMOUNT\").over(customer_window)\n",
        ").withColumn(\n",
        "    \"CUSTOMER_TOTAL_AMOUNT\",\n",
        "    spark_sum(\"TX_AMOUNT\").over(customer_window)\n",
        ").withColumn(\n",
        "    \"CUSTOMER_STD_AMOUNT\",\n",
        "    stddev(\"TX_AMOUNT\").over(customer_window)\n",
        ")\n",
        "\n",
        "# Tính số lượng giao dịch fraud trước đó của customer\n",
        "fraud_window = Window.partitionBy(\"CUSTOMER_ID\").rowsBetween(Window.unboundedPreceding, Window.currentRow - 1)\n",
        "df_with_customer_features = df_with_customer_features.withColumn(\n",
        "    \"CUSTOMER_PREV_FRAUD_COUNT\",\n",
        "    spark_sum((col(\"TX_FRAUD\") == 1).cast(\"int\")).over(fraud_window)\n",
        ")\n",
        "\n",
        "print(\"Customer-level features đã được tạo:\")\n",
        "df_with_customer_features.select(\n",
        "    \"CUSTOMER_ID\", \"TX_AMOUNT\", \"TX_FRAUD\",\n",
        "    \"CUSTOMER_TX_COUNT\", \"CUSTOMER_AVG_AMOUNT\", \n",
        "    \"CUSTOMER_TOTAL_AMOUNT\", \"CUSTOMER_STD_AMOUNT\",\n",
        "    \"CUSTOMER_PREV_FRAUD_COUNT\"\n",
        ").show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Feature Engineering: Terminal-level Statistics\n",
        "# ============================================\n",
        "# Window function để tính statistics theo terminal\n",
        "terminal_window = Window.partitionBy(\"TERMINAL_ID\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "df_with_terminal_features = df_with_customer_features.withColumn(\n",
        "    \"TERMINAL_TX_COUNT\",\n",
        "    spark_count(\"*\").over(terminal_window)\n",
        ").withColumn(\n",
        "    \"TERMINAL_AVG_AMOUNT\",\n",
        "    avg(\"TX_AMOUNT\").over(terminal_window)\n",
        ").withColumn(\n",
        "    \"TERMINAL_FRAUD_RATE\",\n",
        "    avg((col(\"TX_FRAUD\") == 1).cast(\"double\")).over(terminal_window)\n",
        ")\n",
        "\n",
        "print(\"Terminal-level features đã được tạo:\")\n",
        "df_with_terminal_features.select(\n",
        "    \"TERMINAL_ID\", \"TX_AMOUNT\", \"TX_FRAUD\",\n",
        "    \"TERMINAL_TX_COUNT\", \"TERMINAL_AVG_AMOUNT\", \"TERMINAL_FRAUD_RATE\"\n",
        ").show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Feature Engineering: Time-based Features\n",
        "# ============================================\n",
        "from pyspark.sql.functions import when, log\n",
        "\n",
        "# Tạo các time-based features\n",
        "df_final_features = df_with_terminal_features.withColumn(\n",
        "    \"IS_WEEKEND\",\n",
        "    when((col(\"TX_DAYOFWEEK\") == 1) | (col(\"TX_DAYOFWEEK\") == 7), 1).otherwise(0)\n",
        ").withColumn(\n",
        "    \"IS_NIGHT\",\n",
        "    when((col(\"TX_HOUR\") >= 22) | (col(\"TX_HOUR\") <= 6), 1).otherwise(0)\n",
        ").withColumn(\n",
        "    \"IS_BUSINESS_HOURS\",\n",
        "    when((col(\"TX_HOUR\") >= 9) & (col(\"TX_HOUR\") <= 17), 1).otherwise(0)\n",
        ").withColumn(\n",
        "    \"TX_AMOUNT_LOG\",\n",
        "    log(col(\"TX_AMOUNT\") + 1)  # Log transform để giảm skewness\n",
        ")\n",
        "\n",
        "print(\"Time-based và transformed features đã được tạo:\")\n",
        "df_final_features.select(\n",
        "    \"TX_HOUR\", \"TX_DAYOFWEEK\", \"TX_AMOUNT\", \"TX_AMOUNT_LOG\",\n",
        "    \"IS_WEEKEND\", \"IS_NIGHT\", \"IS_BUSINESS_HOURS\"\n",
        ").show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Scaling và Preprocessing\n",
        "\n",
        "Chuẩn hóa các numerical features để cải thiện hiệu suất của các thuật toán machine learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Feature Scaling với StandardScaler\n",
        "# ============================================\n",
        "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
        "\n",
        "# Cập nhật danh sách features để bao gồm các features mới\n",
        "feature_cols = [\n",
        "    \"CUSTOMER_ID\",\n",
        "    \"TERMINAL_ID\",\n",
        "    \"TX_AMOUNT\",\n",
        "    \"TX_TIME_SECONDS\",\n",
        "    \"TX_TIME_DAYS\",\n",
        "    \"TX_HOUR\",\n",
        "    \"TX_DAYOFWEEK\",\n",
        "    \"CUSTOMER_TX_COUNT\",\n",
        "    \"CUSTOMER_AVG_AMOUNT\",\n",
        "    \"CUSTOMER_TOTAL_AMOUNT\",\n",
        "    \"CUSTOMER_STD_AMOUNT\",\n",
        "    \"CUSTOMER_PREV_FRAUD_COUNT\",\n",
        "    \"TERMINAL_TX_COUNT\",\n",
        "    \"TERMINAL_AVG_AMOUNT\",\n",
        "    \"TERMINAL_FRAUD_RATE\",\n",
        "    \"IS_WEEKEND\",\n",
        "    \"IS_NIGHT\",\n",
        "    \"IS_BUSINESS_HOURS\",\n",
        "    \"TX_AMOUNT_LOG\"\n",
        "]\n",
        "\n",
        "# Tạo vector assembler\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=feature_cols,\n",
        "    outputCol=\"features_raw\",\n",
        "    handleInvalid=\"keep\"\n",
        ")\n",
        "\n",
        "# Transform data\n",
        "df_assembled = assembler.transform(df_final_features)\n",
        "\n",
        "# Apply StandardScaler\n",
        "scaler = StandardScaler(\n",
        "    inputCol=\"features_raw\",\n",
        "    outputCol=\"features\",\n",
        "    withMean=True,\n",
        "    withStd=True\n",
        ")\n",
        "\n",
        "scaler_model = scaler.fit(df_assembled)\n",
        "df_scaled = scaler_model.transform(df_assembled)\n",
        "\n",
        "# Tạo DataFrame cho ML\n",
        "df_ml = df_scaled.select(\"features\", \"TX_FRAUD\")\n",
        "\n",
        "print(\"Features đã được scaled:\")\n",
        "print(f\"Số lượng features: {len(feature_cols)}\")\n",
        "df_ml.show(3, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cải Thiện Model Evaluation - Đầy Đủ Metrics\n",
        "\n",
        "Tạo hàm để tính toán tất cả các metrics quan trọng cho classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Hàm tính toán đầy đủ metrics\n",
        "# ============================================\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from sklearn.metrics import precision_recall_curve, auc as sklearn_auc\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_all_metrics(predictions, label_col=\"TX_FRAUD\", prediction_col=\"prediction\", prob_col=\"probability\"):\n",
        "    \"\"\"\n",
        "    Tính toán tất cả các metrics cho binary classification\n",
        "    \"\"\"\n",
        "    # Binary Classification Metrics\n",
        "    auc_evaluator = BinaryClassificationEvaluator(\n",
        "        labelCol=label_col,\n",
        "        metricName=\"areaUnderROC\"\n",
        "    )\n",
        "    auc = auc_evaluator.evaluate(predictions)\n",
        "    \n",
        "    # Multiclass Classification Metrics\n",
        "    evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=label_col,\n",
        "        predictionCol=prediction_col\n",
        "    )\n",
        "    \n",
        "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
        "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "    f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
        "    \n",
        "    # Tính Precision, Recall, F1 cho từng class\n",
        "    pdf = predictions.select(label_col, prediction_col, prob_col).toPandas()\n",
        "    y_true = pdf[label_col].values\n",
        "    y_pred = pdf[prediction_col].values\n",
        "    \n",
        "    # Confusion matrix\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    # Metrics cho class 1 (Fraud)\n",
        "    if len(cm) == 2:\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        precision_class1 = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall_class1 = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1_class1 = 2 * (precision_class1 * recall_class1) / (precision_class1 + recall_class1) if (precision_class1 + recall_class1) > 0 else 0\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    else:\n",
        "        precision_class1 = recall_class1 = f1_class1 = specificity = 0\n",
        "    \n",
        "    # Precision-Recall AUC\n",
        "    y_proba = pdf[prob_col].apply(lambda x: x[1] if len(x) > 1 else x[0]).values\n",
        "    pr_precision, pr_recall, _ = precision_recall_curve(y_true, y_proba)\n",
        "    pr_auc = sklearn_auc(pr_recall, pr_precision)\n",
        "    \n",
        "    return {\n",
        "        \"AUC-ROC\": auc,\n",
        "        \"AUC-PR\": pr_auc,\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision (Weighted)\": precision,\n",
        "        \"Recall (Weighted)\": recall,\n",
        "        \"F1-Score (Weighted)\": f1,\n",
        "        \"Precision (Fraud Class)\": precision_class1,\n",
        "        \"Recall (Fraud Class)\": recall_class1,\n",
        "        \"F1-Score (Fraud Class)\": f1_class1,\n",
        "        \"Specificity\": specificity,\n",
        "        \"Confusion Matrix\": cm\n",
        "    }\n",
        "\n",
        "def print_metrics(metrics, model_name):\n",
        "    \"\"\"In metrics một cách đẹp mắt\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"METRICS CHO {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"AUC-ROC:              {metrics['AUC-ROC']:.4f}\")\n",
        "    print(f\"AUC-PR:                {metrics['AUC-PR']:.4f}\")\n",
        "    print(f\"Accuracy:              {metrics['Accuracy']:.4f}\")\n",
        "    print(f\"Precision (Weighted):  {metrics['Precision (Weighted)']:.4f}\")\n",
        "    print(f\"Recall (Weighted):     {metrics['Recall (Weighted)']:.4f}\")\n",
        "    print(f\"F1-Score (Weighted):   {metrics['F1-Score (Weighted)']:.4f}\")\n",
        "    print(f\"\\n--- Metrics cho Fraud Class (Class 1) ---\")\n",
        "    print(f\"Precision:             {metrics['Precision (Fraud Class)']:.4f}\")\n",
        "    print(f\"Recall:                {metrics['Recall (Fraud Class)']:.4f}\")\n",
        "    print(f\"F1-Score:              {metrics['F1-Score (Fraud Class)']:.4f}\")\n",
        "    print(f\"Specificity:           {metrics['Specificity']:.4f}\")\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(metrics['Confusion Matrix'])\n",
        "\n",
        "print(\"Hàm calculate_all_metrics đã được định nghĩa!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train/Test Split\n",
        "\n",
        "Chia dataset thành training và testing sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/Test Split với df_ml đã được scaled\n",
        "train_df, test_df = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "print(\"Train:\", train_df.count())\n",
        "print(\"Test :\", test_df.count())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline Models với Đầy Đủ Metrics\n",
        "\n",
        "Train và evaluate baseline models với tất cả metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Baseline Logistic Regression\n",
        "# ============================================\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"TX_FRAUD\",\n",
        "    maxIter=20\n",
        ")\n",
        "\n",
        "lr_model = lr.fit(train_df)\n",
        "lr_pred = lr_model.transform(test_df)\n",
        "\n",
        "# Tính đầy đủ metrics\n",
        "lr_metrics = calculate_all_metrics(lr_pred, label_col=\"TX_FRAUD\")\n",
        "print_metrics(lr_metrics, \"Logistic Regression (Baseline)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Baseline Random Forest\n",
        "# ============================================\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"TX_FRAUD\",\n",
        "    seed=42,\n",
        "    numTrees=100\n",
        ")\n",
        "\n",
        "rf_model = rf.fit(train_df)\n",
        "rf_pred = rf_model.transform(test_df)\n",
        "\n",
        "# Tính đầy đủ metrics\n",
        "rf_metrics = calculate_all_metrics(rf_pred, label_col=\"TX_FRAUD\")\n",
        "print_metrics(rf_metrics, \"Random Forest (Baseline)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter Tuning với Grid Search\n",
        "\n",
        "Sử dụng Grid Search để tìm hyperparameters tối ưu cho các models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Hyperparameter Tuning cho Logistic Regression\n",
        "# ============================================\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "import time\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"HYPERPARAMETER TUNING CHO LOGISTIC REGRESSION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Tạo parameter grid\n",
        "lr_param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
        "    .build()\n",
        "\n",
        "# Evaluator\n",
        "evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=\"TX_FRAUD\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "\n",
        "# Cross Validator\n",
        "lr_cv = CrossValidator(\n",
        "    estimator=lr,\n",
        "    estimatorParamMaps=lr_param_grid,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=3,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Fit model với cross-validation\n",
        "print(\"\\nBắt đầu Grid Search cho Logistic Regression...\")\n",
        "start_time = time.time()\n",
        "lr_cv_model = lr_cv.fit(train_df)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Hoàn thành trong {end_time - start_time:.2f} giây\")\n",
        "\n",
        "# Best model\n",
        "lr_best_model = lr_cv_model.bestModel\n",
        "print(f\"\\nBest parameters:\")\n",
        "print(f\"  regParam: {lr_best_model.getRegParam()}\")\n",
        "print(f\"  elasticNetParam: {lr_best_model.getElasticNetParam()}\")\n",
        "\n",
        "# Evaluate best model\n",
        "lr_tuned_pred = lr_best_model.transform(test_df)\n",
        "lr_tuned_metrics = calculate_all_metrics(lr_tuned_pred, label_col=\"TX_FRAUD\")\n",
        "print_metrics(lr_tuned_metrics, \"Logistic Regression (Tuned)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Hyperparameter Tuning cho Random Forest\n",
        "# ============================================\n",
        "print(\"=\" * 60)\n",
        "print(\"HYPERPARAMETER TUNING CHO RANDOM FOREST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Tạo parameter grid cho Random Forest\n",
        "rf_param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(rf.numTrees, [50, 100, 200]) \\\n",
        "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
        "    .addGrid(rf.maxBins, [32, 64]) \\\n",
        "    .build()\n",
        "\n",
        "# Cross Validator cho Random Forest\n",
        "rf_cv = CrossValidator(\n",
        "    estimator=rf,\n",
        "    estimatorParamMaps=rf_param_grid,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=3,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Fit model với cross-validation\n",
        "print(\"\\nBắt đầu Grid Search cho Random Forest...\")\n",
        "print(\"(Lưu ý: Quá trình này có thể mất nhiều thời gian)\")\n",
        "start_time = time.time()\n",
        "rf_cv_model = rf_cv.fit(train_df)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Hoàn thành trong {end_time - start_time:.2f} giây\")\n",
        "\n",
        "# Best model\n",
        "rf_best_model = rf_cv_model.bestModel\n",
        "print(f\"\\nBest parameters:\")\n",
        "print(f\"  numTrees: {rf_best_model.getNumTrees}\")\n",
        "print(f\"  maxDepth: {rf_best_model.getMaxDepth()}\")\n",
        "print(f\"  maxBins: {rf_best_model.getMaxBins()}\")\n",
        "\n",
        "# Evaluate best model\n",
        "rf_tuned_pred = rf_best_model.transform(test_df)\n",
        "rf_tuned_metrics = calculate_all_metrics(rf_tuned_pred, label_col=\"TX_FRAUD\")\n",
        "print_metrics(rf_tuned_metrics, \"Random Forest (Tuned)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Importance Analysis\n",
        "\n",
        "Phân tích tầm quan trọng của các features để hiểu model tốt hơn.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Feature Importance từ Random Forest\n",
        "# ============================================\n",
        "import numpy as np\n",
        "\n",
        "# Lấy feature importance từ Random Forest model\n",
        "rf_feature_importance = rf_best_model.featureImportances.toArray()\n",
        "\n",
        "# Tạo DataFrame với feature names và importance\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_cols,\n",
        "    'Importance': rf_feature_importance\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FEATURE IMPORTANCE (Random Forest)\")\n",
        "print(\"=\" * 60)\n",
        "print(feature_importance_df.to_string(index=False))\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_features = feature_importance_df.head(15)\n",
        "plt.barh(range(len(top_features)), top_features['Importance'], color='steelblue')\n",
        "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Top 15 Feature Importance (Random Forest)')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Coefficients từ Logistic Regression\n",
        "# ============================================\n",
        "lr_coefficients = lr_best_model.coefficients.toArray()\n",
        "\n",
        "# Tạo DataFrame với feature names và coefficients\n",
        "coefficients_df = pd.DataFrame({\n",
        "    'Feature': feature_cols,\n",
        "    'Coefficient': lr_coefficients\n",
        "}).sort_values('Coefficient', key=abs, ascending=False)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FEATURE COEFFICIENTS (Logistic Regression)\")\n",
        "print(\"=\" * 60)\n",
        "print(coefficients_df.to_string(index=False))\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_coef = coefficients_df.head(15)\n",
        "colors = ['red' if x < 0 else 'green' for x in top_coef['Coefficient']]\n",
        "plt.barh(range(len(top_coef)), top_coef['Coefficient'], color=colors)\n",
        "plt.yticks(range(len(top_coef)), top_coef['Feature'])\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.title('Top 15 Feature Coefficients (Logistic Regression)')\n",
        "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Precision-Recall Curves\n",
        "\n",
        "Vẽ Precision-Recall curves cho tất cả models để so sánh hiệu suất trên imbalanced data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Precision-Recall Curves cho tất cả models\n",
        "# ============================================\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "def get_pr_data(spark_df, label_col=\"TX_FRAUD\", prob_col=\"probability\"):\n",
        "    \"\"\"Lấy data cho Precision-Recall curve\"\"\"\n",
        "    pdf = spark_df.select(label_col, prob_col).toPandas()\n",
        "    y_true = pdf[label_col].values\n",
        "    y_proba = pdf[prob_col].apply(lambda x: x[1] if len(x) > 1 else x[0]).values\n",
        "    return y_true, y_proba\n",
        "\n",
        "# Lấy data cho các models\n",
        "lr_y_true, lr_y_proba = get_pr_data(lr_pred)\n",
        "rf_y_true, rf_y_proba = get_pr_data(rf_pred)\n",
        "lr_tuned_y_true, lr_tuned_y_proba = get_pr_data(lr_tuned_pred)\n",
        "rf_tuned_y_true, rf_tuned_y_proba = get_pr_data(rf_tuned_pred)\n",
        "\n",
        "# Tính Precision-Recall curves\n",
        "lr_precision, lr_recall, _ = precision_recall_curve(lr_y_true, lr_y_proba)\n",
        "rf_precision, rf_recall, _ = precision_recall_curve(rf_y_true, rf_y_proba)\n",
        "lr_tuned_precision, lr_tuned_recall, _ = precision_recall_curve(lr_tuned_y_true, lr_tuned_y_proba)\n",
        "rf_tuned_precision, rf_tuned_recall, _ = precision_recall_curve(rf_tuned_y_true, rf_tuned_y_proba)\n",
        "\n",
        "# Tính AUC-PR\n",
        "lr_pr_auc = sklearn_auc(lr_recall, lr_precision)\n",
        "rf_pr_auc = sklearn_auc(rf_recall, rf_precision)\n",
        "lr_tuned_pr_auc = sklearn_auc(lr_tuned_recall, lr_tuned_precision)\n",
        "rf_tuned_pr_auc = sklearn_auc(rf_tuned_recall, rf_tuned_precision)\n",
        "\n",
        "# Vẽ Precision-Recall curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(lr_recall, lr_precision, color='blue', lw=2, \n",
        "         label=f'LR Baseline (AUC-PR = {lr_pr_auc:.3f})')\n",
        "plt.plot(rf_recall, rf_precision, color='green', lw=2, \n",
        "         label=f'RF Baseline (AUC-PR = {rf_pr_auc:.3f})')\n",
        "plt.plot(lr_tuned_recall, lr_tuned_precision, color='red', lw=2, \n",
        "         label=f'LR Tuned (AUC-PR = {lr_tuned_pr_auc:.3f})')\n",
        "plt.plot(rf_tuned_recall, rf_tuned_precision, color='purple', lw=2, \n",
        "         label=f'RF Tuned (AUC-PR = {rf_tuned_pr_auc:.3f})')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curves Comparison')\n",
        "plt.legend(loc='lower left')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spark Performance Optimization\n",
        "\n",
        "Tối ưu hóa cấu hình Spark để cải thiện hiệu suất.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Spark Performance Optimization\n",
        "# ============================================\n",
        "print(\"=\" * 60)\n",
        "print(\"SPARK CONFIGURATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Hiển thị current Spark config\n",
        "spark_conf = spark.sparkContext.getConf()\n",
        "print(f\"\\nSpark App Name: {spark_conf.get('spark.app.name')}\")\n",
        "print(f\"Spark Master: {spark_conf.get('spark.master', 'local[*]')}\")\n",
        "\n",
        "# Cache DataFrames để tăng tốc độ\n",
        "print(\"\\nCaching training và test DataFrames...\")\n",
        "train_df.cache()\n",
        "test_df.cache()\n",
        "\n",
        "# Kiểm tra số partitions\n",
        "print(f\"\\nSố partitions của train_df: {train_df.rdd.getNumPartitions()}\")\n",
        "print(f\"Số partitions của test_df: {test_df.rdd.getNumPartitions()}\")\n",
        "\n",
        "# Đếm để trigger caching\n",
        "print(\"\\nĐang cache DataFrames (lần đầu sẽ chậm)...\")\n",
        "train_count = train_df.count()\n",
        "test_count = test_df.count()\n",
        "print(f\"Train count: {train_count:,}\")\n",
        "print(f\"Test count: {test_count:,}\")\n",
        "print(\"\\n✓ DataFrames đã được cache. Các operations tiếp theo sẽ nhanh hơn.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## So Sánh Tổng Hợp Tất Cả Models\n",
        "\n",
        "Tạo bảng so sánh tổng hợp cho tất cả models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# So sánh tổng hợp tất cả models\n",
        "# ============================================\n",
        "print(\"=\" * 60)\n",
        "print(\"SO SÁNH TỔNG HỢP TẤT CẢ MODELS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Tạo comparison DataFrame\n",
        "comparison_data = {\n",
        "    'Model': [\n",
        "        'LR Baseline',\n",
        "        'RF Baseline',\n",
        "        'LR Tuned',\n",
        "        'RF Tuned'\n",
        "    ],\n",
        "    'AUC-ROC': [\n",
        "        lr_metrics['AUC-ROC'],\n",
        "        rf_metrics['AUC-ROC'],\n",
        "        lr_tuned_metrics['AUC-ROC'],\n",
        "        rf_tuned_metrics['AUC-ROC']\n",
        "    ],\n",
        "    'AUC-PR': [\n",
        "        lr_metrics['AUC-PR'],\n",
        "        rf_metrics['AUC-PR'],\n",
        "        lr_tuned_metrics['AUC-PR'],\n",
        "        rf_tuned_metrics['AUC-PR']\n",
        "    ],\n",
        "    'Accuracy': [\n",
        "        lr_metrics['Accuracy'],\n",
        "        rf_metrics['Accuracy'],\n",
        "        lr_tuned_metrics['Accuracy'],\n",
        "        rf_tuned_metrics['Accuracy']\n",
        "    ],\n",
        "    'Precision (Fraud)': [\n",
        "        lr_metrics['Precision (Fraud Class)'],\n",
        "        rf_metrics['Precision (Fraud Class)'],\n",
        "        lr_tuned_metrics['Precision (Fraud Class)'],\n",
        "        rf_tuned_metrics['Precision (Fraud Class)']\n",
        "    ],\n",
        "    'Recall (Fraud)': [\n",
        "        lr_metrics['Recall (Fraud Class)'],\n",
        "        rf_metrics['Recall (Fraud Class)'],\n",
        "        lr_tuned_metrics['Recall (Fraud Class)'],\n",
        "        rf_tuned_metrics['Recall (Fraud Class)']\n",
        "    ],\n",
        "    'F1-Score (Fraud)': [\n",
        "        lr_metrics['F1-Score (Fraud Class)'],\n",
        "        rf_metrics['F1-Score (Fraud Class)'],\n",
        "        lr_tuned_metrics['F1-Score (Fraud Class)'],\n",
        "        rf_tuned_metrics['F1-Score (Fraud Class)']\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\n\" + comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# AUC-ROC và AUC-PR\n",
        "axes[0, 0].bar(comparison_df['Model'], comparison_df['AUC-ROC'], color='steelblue', alpha=0.7)\n",
        "axes[0, 0].set_ylabel('AUC-ROC')\n",
        "axes[0, 0].set_title('AUC-ROC Comparison')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0, 1].bar(comparison_df['Model'], comparison_df['AUC-PR'], color='coral', alpha=0.7)\n",
        "axes[0, 1].set_ylabel('AUC-PR')\n",
        "axes[0, 1].set_title('AUC-PR Comparison')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Precision, Recall, F1\n",
        "x = range(len(comparison_df))\n",
        "width = 0.25\n",
        "axes[1, 0].bar([i - width for i in x], comparison_df['Precision (Fraud)'], width, \n",
        "                label='Precision', color='green', alpha=0.7)\n",
        "axes[1, 0].bar(x, comparison_df['Recall (Fraud)'], width, \n",
        "               label='Recall', color='blue', alpha=0.7)\n",
        "axes[1, 0].bar([i + width for i in x], comparison_df['F1-Score (Fraud)'], width, \n",
        "               label='F1-Score', color='red', alpha=0.7)\n",
        "axes[1, 0].set_ylabel('Score')\n",
        "axes[1, 0].set_title('Precision, Recall, F1-Score (Fraud Class)')\n",
        "axes[1, 0].set_xticks(x)\n",
        "axes[1, 0].set_xticklabels(comparison_df['Model'], rotation=45)\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy\n",
        "axes[1, 1].bar(comparison_df['Model'], comparison_df['Accuracy'], color='purple', alpha=0.7)\n",
        "axes[1, 1].set_ylabel('Accuracy')\n",
        "axes[1, 1].set_title('Accuracy Comparison')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Tìm best model\n",
        "best_model_idx = comparison_df['F1-Score (Fraud)'].idxmax()\n",
        "best_model = comparison_df.loc[best_model_idx, 'Model']\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"BEST MODEL: {best_model}\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"F1-Score (Fraud): {comparison_df.loc[best_model_idx, 'F1-Score (Fraud)']:.4f}\")\n",
        "print(f\"Recall (Fraud): {comparison_df.loc[best_model_idx, 'Recall (Fraud)']:.4f}\")\n",
        "print(f\"Precision (Fraud): {comparison_df.loc[best_model_idx, 'Precision (Fraud)']:.4f}\")\n",
        "print(f\"AUC-ROC: {comparison_df.loc[best_model_idx, 'AUC-ROC']:.4f}\")\n",
        "print(f\"AUC-PR: {comparison_df.loc[best_model_idx, 'AUC-PR']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "wK5GN777_6uJ",
        "outputId": "d8ae4f78-332a-405c-d4b3-4a9929612b42"
      },
      "outputs": [],
      "source": [
        "### ANALYSE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pdf_label = df_clean.select(\"TX_FRAUD\").toPandas()\n",
        "\n",
        "plt.figure()\n",
        "pdf_label[\"TX_FRAUD\"].value_counts().plot(kind=\"bar\")\n",
        "plt.title(\"Fraud vs Non-Fraud Distribution\")\n",
        "plt.xlabel(\"TX_FRAUD (0 = Normal, 1 = Fraud)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE5oO__E8rBL"
      },
      "outputs": [],
      "source": [
        "feature_cols = [\n",
        "    \"CUSTOMER_ID\",\n",
        "    \"TERMINAL_ID\",\n",
        "    \"TX_AMOUNT\",\n",
        "    \"TX_TIME_SECONDS\",\n",
        "    \"TX_TIME_DAYS\",\n",
        "    \"TX_HOUR\",\n",
        "    \"TX_DAYOFWEEK\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFmYOW2V9IV-",
        "outputId": "de1481a8-c0c6-492c-e2e7-a62b4fe9a84c"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=feature_cols,\n",
        "    outputCol=\"features\",\n",
        "    handleInvalid=\"keep\"\n",
        ")\n",
        "\n",
        "df_ml = assembler.transform(df_clean) \\\n",
        "    .select(\"features\", \"TX_FRAUD\")\n",
        "\n",
        "df_ml.show(3, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIgG4K_J9MSf",
        "outputId": "1b8e4fba-8f12-4dcc-ba9e-6fc6d750ad70"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "print(\"Train:\", train_df.count())\n",
        "print(\"Test :\", test_df.count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4BnXy4f95HB"
      },
      "outputs": [],
      "source": [
        "### BASE LINE\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"TX_FRAUD\",\n",
        "    maxIter=20\n",
        ")\n",
        "\n",
        "lr_model = lr.fit(train_df)\n",
        "lr_pred = lr_model.transform(test_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB1dXABo-0z2",
        "outputId": "99b4a379-6046-4729-8087-c7b9d4ee7739"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=\"TX_FRAUD\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "\n",
        "auc = evaluator.evaluate(lr_pred)\n",
        "print(\"Logistic Regression AUC:\", auc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mjuqvn6h-4r-",
        "outputId": "87dc1a41-006a-4f0a-8f40-fa61a1fc432f"
      },
      "outputs": [],
      "source": [
        "lr_pred.groupBy(\"TX_FRAUD\", \"prediction\").count().orderBy(\"TX_FRAUD\", \"prediction\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33a544c8"
      },
      "source": [
        "# Perform\n",
        "\n",
        "Perform a comparative analysis of Logistic Regression and Random Forest models for fraud detection using the dataset `Final Transaction.csv`. The analysis should include:\n",
        "1.  **Baseline Evaluation (Before SMOTE)**:\n",
        "    *   Calculate the Recall score for the existing Logistic Regression model.\n",
        "    *   Train a Random Forest Classifier and evaluate its AUC and Recall scores.\n",
        "2.  **SMOTE Application**: Apply SMOTE to balance the training dataset.\n",
        "3.  **Post-SMOTE Evaluation**:\n",
        "    *   Train a Logistic Regression model on the SMOTE-balanced data and evaluate its AUC and Recall scores on the original unbalanced test set.\n",
        "    *   Train a Random Forest model on the SMOTE-balanced data and evaluate its AUC and Recall scores on the original unbalanced test set.\n",
        "4.  **Summary and Comparison**: Present a comprehensive summary of all AUC and Recall scores, comparing the performance of both models before and after SMOTE. Discuss the findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dec0783f"
      },
      "source": [
        "## Calculate Recall for Logistic Regression (Before SMOTE)\n",
        "\n",
        "### Subtask:\n",
        "Calculate and print the Recall score for the existing Logistic Regression model trained on the unbalanced dataset. We already have the predictions (`lr_pred`) and AUC.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45aacbe6"
      },
      "source": [
        "**Reasoning**:\n",
        "To calculate the recall score, I need to use `MulticlassClassificationEvaluator` from `pyspark.ml.evaluation`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecaed8b9",
        "outputId": "9a17c777-bc71-40b3-ca50-a474205a0e64"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator_recall = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"TX_FRAUD\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"recallByLabel\"\n",
        ")\n",
        "\n",
        "# Get recall for each class\n",
        "recall_by_label = evaluator_recall.evaluate(lr_pred)\n",
        "\n",
        "# Extract recall for the positive class (label 1)\n",
        "# Note: evaluate returns a single aggregated metric. To get recall for specific label,\n",
        "# one typically needs to either filter the predictions or configure the evaluator differently.\n",
        "# However, the provided instruction `metricName='recallByLabel'` will return an aggregated recall.\n",
        "# Let's adjust to get the recall for the positive class specifically.\n",
        "\n",
        "# A common way to get per-class metrics is to manually calculate or use a confusion matrix\n",
        "# or iterate through classes if the evaluator supports it (which it does via summary).\n",
        "# Given the request, the simplest way to get recall for label 1 is to filter the data.\n",
        "\n",
        "# For recall of label 1, we only care about actual label 1s that were predicted correctly\n",
        "# True Positives (TX_FRAUD=1, prediction=1) / (True Positives + False Negatives)\n",
        "# False Negatives (TX_FRAUD=1, prediction=0)\n",
        "\n",
        "true_positives = lr_pred.filter((lr_pred.TX_FRAUD == 1) & (lr_pred.prediction == 1)).count()\n",
        "false_negatives = lr_pred.filter((lr_pred.TX_FRAUD == 1) & (lr_pred.prediction == 0)).count()\n",
        "\n",
        "recall_label_1 = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "\n",
        "print(f\"Logistic Regression Recall for Class 1 (Fraud): {recall_label_1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50cf616e"
      },
      "source": [
        "## Train and Evaluate Random Forest (Before SMOTE)\n",
        "\n",
        "### Subtask:\n",
        "Train a Random Forest Classifier on the original unbalanced training dataset (`train_df`), make predictions on the test set (`test_df`), and then calculate both AUC and Recall scores for this model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8b07037"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to train a Random Forest Classifier, make predictions, and then calculate its AUC and Recall scores as per the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ce77797",
        "outputId": "3512d8d7-4822-463c-bf96-695a562b5373"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# 1. Instantiate a RandomForestClassifier\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"TX_FRAUD\",\n",
        "    seed=42,\n",
        "    numTrees=100 # Default numTrees is 20, using 100 for better performance\n",
        ")\n",
        "\n",
        "# 2. Fit the Random Forest model to the train_df DataFrame\n",
        "rf_model = rf.fit(train_df)\n",
        "\n",
        "# 3. Make predictions on the test_df DataFrame\n",
        "rf_pred = rf_model.transform(test_df)\n",
        "\n",
        "# Display some predictions\n",
        "rf_pred.select(\"TX_FRAUD\", \"prediction\", \"probability\").show(5)\n",
        "\n",
        "# 4. Instantiate a BinaryClassificationEvaluator for AUC\n",
        "auc_evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=\"TX_FRAUD\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "\n",
        "# 5. Evaluate the rf_pred DataFrame to calculate the AUC score\n",
        "rf_auc = auc_evaluator.evaluate(rf_pred)\n",
        "print(f\"Random Forest AUC (Before SMOTE): {rf_auc}\")\n",
        "\n",
        "# 6. Calculate the Recall score for the fraud class (label 1)\n",
        "rf_true_positives = rf_pred.filter((rf_pred.TX_FRAUD == 1) & (rf_pred.prediction == 1)).count()\n",
        "rf_false_negatives = rf_pred.filter((rf_pred.TX_FRAUD == 1) & (rf_pred.prediction == 0)).count()\n",
        "\n",
        "rf_recall_label_1 = rf_true_positives / (rf_true_positives + rf_false_negatives) if (rf_true_positives + rf_false_negatives) > 0 else 0\n",
        "\n",
        "# 7. Print the calculated Recall score\n",
        "print(f\"Random Forest Recall for Class 1 (Fraud) (Before SMOTE): {rf_recall_label_1}\")\n",
        "\n",
        "# Show confusion matrix for RF model\n",
        "rf_pred.groupBy(\"TX_FRAUD\", \"prediction\").count().orderBy(\"TX_FRAUD\", \"prediction\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff3916d8"
      },
      "source": [
        "## Apply SMOTE to Training Data\n",
        "\n",
        "### Subtask:\n",
        "Convert the Spark training DataFrame (`train_df`) to a Pandas DataFrame, apply the SMOTE (Synthetic Minority Over-sampling Technique) algorithm to balance the classes, and then convert the balanced data back into a Spark DataFrame. This balanced dataset will be used for training subsequent models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fda9a2c3"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step in applying SMOTE is to ensure the `imblearn` library is installed, which is not available by default in most environments. This needs to be done in a separate code block using `!pip install`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "290ba3ad",
        "outputId": "194c0100-acc8-4136-d57b-c067274b402a"
      },
      "outputs": [],
      "source": [
        "!pip install imblearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7886bae1"
      },
      "source": [
        "**Reasoning**:\n",
        "The `imblearn` library has been installed. Now, I will proceed with the remaining instructions of the subtask: converting the Spark DataFrame to Pandas, applying SMOTE, converting back to Spark, and verifying the balancing. All these steps are related to the SMOTE application and can be performed in one code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a370f2c9",
        "outputId": "b2934b00-a2c9-402e-8fee-37ed10637eef"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# SMOTE for Spark Training Data\n",
        "# =========================\n",
        "\n",
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1. Convert Spark DataFrame -> Pandas\n",
        "# -------------------------------------------------\n",
        "# train_df must contain: [features: VectorUDT, TX_FRAUD: int]\n",
        "\n",
        "pd_train_df = train_df.select(\"features\", \"TX_FRAUD\").toPandas()\n",
        "\n",
        "# Convert Spark VectorUDT -> Pandas numerical matrix\n",
        "X_pd = pd.DataFrame(\n",
        "    pd_train_df[\"features\"].apply(lambda v: v.toArray()).tolist()\n",
        ")\n",
        "y_pd = pd_train_df[\"TX_FRAUD\"]\n",
        "\n",
        "print(\"Original class distribution:\")\n",
        "print(y_pd.value_counts())\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2. Apply SMOTE\n",
        "# -------------------------------------------------\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X_pd, y_pd)\n",
        "\n",
        "print(\"\\nAfter SMOTE class distribution:\")\n",
        "print(pd.Series(y_res).value_counts())\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3. Convert back to Spark DataFrame\n",
        "# -------------------------------------------------\n",
        "data_for_spark = [\n",
        "    (Vectors.dense(X_res.iloc[i].values.tolist()), int(y_res.iloc[i]))\n",
        "    for i in range(len(y_res))\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"features\", VectorUDT(), True),\n",
        "    StructField(\"TX_FRAUD\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "train_df_smote = spark.createDataFrame(data_for_spark, schema)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4. Verification\n",
        "# -------------------------------------------------\n",
        "print(\"\\nSpark SMOTE DataFrame:\")\n",
        "train_df_smote.printSchema()\n",
        "\n",
        "print(\"Row count:\", train_df_smote.count())\n",
        "train_df_smote.groupBy(\"TX_FRAUD\").count().show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdc6c45d"
      },
      "source": [
        "## Train Logistic Regression (After SMOTE)\n",
        "\n",
        "### Subtask:\n",
        "Train a new Logistic Regression model using the SMOTE-balanced training dataset created in the previous step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86722ab3"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires training a Logistic Regression model on the SMOTE-balanced data, which involves instantiating the model with specified parameters and then fitting it to the `train_df_smote` DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49704247",
        "outputId": "8ed2d9df-44a3-4990-a6b3-fd505656a8cc"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# 1. Instantiate a LogisticRegression model\n",
        "lr_smote = LogisticRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"TX_FRAUD\",\n",
        "    maxIter=20\n",
        ")\n",
        "\n",
        "# 2. Fit this Logistic Regression model to the train_df_smote DataFrame\n",
        "lr_model_smote = lr_smote.fit(train_df_smote)\n",
        "\n",
        "print(\"Logistic Regression model trained on SMOTE-balanced data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaa30615"
      },
      "source": [
        "## Evaluate Logistic Regression (After SMOTE)\n",
        "\n",
        "### Subtask:\n",
        "Make predictions on the original unbalanced `test_df` using the `lr_model_smote`, then calculate and print its AUC and Recall scores for the fraud class (label 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4352c099"
      },
      "source": [
        "**Reasoning**:\n",
        "To evaluate the Logistic Regression model trained on SMOTE-balanced data, I need to make predictions on the original test set and then compute the AUC and Recall scores, similar to how the baseline Logistic Regression model was evaluated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48e91ae4",
        "outputId": "e87dee88-a4d0-4170-d0e4-94acca220210"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# 1. Make predictions on the original test_df DataFrame using the lr_model_smote\n",
        "lr_smote_pred = lr_model_smote.transform(test_df)\n",
        "\n",
        "# Display some predictions\n",
        "lr_smote_pred.select(\"TX_FRAUD\", \"prediction\", \"probability\").show(5)\n",
        "\n",
        "# 2. Instantiate a BinaryClassificationEvaluator for AUC\n",
        "auc_evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=\"TX_FRAUD\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "\n",
        "# 3. Evaluate the lr_smote_pred DataFrame to calculate the AUC score\n",
        "lr_smote_auc = auc_evaluator.evaluate(lr_smote_pred)\n",
        "print(f\"Logistic Regression AUC (After SMOTE): {lr_smote_auc}\")\n",
        "\n",
        "# 4. Calculate the Recall score for the fraud class (label 1)\n",
        "lr_smote_true_positives = lr_smote_pred.filter((lr_smote_pred.TX_FRAUD == 1) & (lr_smote_pred.prediction == 1)).count()\n",
        "lr_smote_false_negatives = lr_smote_pred.filter((lr_smote_pred.TX_FRAUD == 1) & (lr_smote_pred.prediction == 0)).count()\n",
        "\n",
        "lr_smote_recall_label_1 = lr_smote_true_positives / (lr_smote_true_positives + lr_smote_false_negatives) if (lr_smote_true_positives + lr_smote_false_negatives) > 0 else 0\n",
        "\n",
        "# 5. Print the calculated Recall score\n",
        "print(f\"Logistic Regression Recall for Class 1 (Fraud) (After SMOTE): {lr_smote_recall_label_1}\")\n",
        "\n",
        "# Show confusion matrix for LR model after SMOTE\n",
        "lr_smote_pred.groupBy(\"TX_FRAUD\", \"prediction\").count().orderBy(\"TX_FRAUD\", \"prediction\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47e2feb8"
      },
      "source": [
        "## Train Random Forest (After SMOTE)\n",
        "\n",
        "### Subtask:\n",
        "Train a new Random Forest Classifier model using the SMOTE-balanced training dataset (`train_df_smote`) created earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05cfba8d"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the previous markdown block, the next logical step is to implement the training of the Random Forest Classifier on the SMOTE-balanced dataset. This involves instantiating the model with appropriate parameters and then fitting it to the `train_df_smote` DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed9a0a74",
        "outputId": "1781f4e1-5463-438a-878e-a39b652cf18e"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "# 1. Instantiate a RandomForestClassifier\n",
        "rf_smote = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"TX_FRAUD\",\n",
        "    seed=42,\n",
        "    numTrees=100 # Consistent with baseline RF model\n",
        ")\n",
        "\n",
        "# 2. Fit the Random Forest model to the train_df_smote DataFrame\n",
        "rf_model_smote = rf_smote.fit(train_df_smote)\n",
        "\n",
        "print(\"Random Forest model trained on SMOTE-balanced data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94f8421b"
      },
      "source": [
        "## Evaluate Random Forest (After SMOTE)\n",
        "\n",
        "### Subtask:\n",
        "Make predictions on the original unbalanced `test_df` using the `rf_model_smote`, then calculate and print its AUC and Recall scores for the fraud class (label 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f39ef862"
      },
      "source": [
        "**Reasoning**:\n",
        "To evaluate the Random Forest model trained on SMOTE-balanced data, I need to make predictions on the original test set and then compute the AUC and Recall scores, similar to how the previous models were evaluated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7607d927",
        "outputId": "85219430-3639-48dd-cbea-6e0ea2075f6e"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# 1. Make predictions on the original test_df DataFrame using the rf_model_smote\n",
        "rf_smote_pred = rf_model_smote.transform(test_df)\n",
        "\n",
        "# Display some predictions\n",
        "rf_smote_pred.select(\"TX_FRAUD\", \"prediction\", \"probability\").show(5)\n",
        "\n",
        "# 2. Instantiate a BinaryClassificationEvaluator for AUC\n",
        "auc_evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=\"TX_FRAUD\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "\n",
        "# 3. Evaluate the rf_smote_pred DataFrame to calculate the AUC score\n",
        "rf_smote_auc = auc_evaluator.evaluate(rf_smote_pred)\n",
        "print(f\"Random Forest AUC (After SMOTE): {rf_smote_auc}\")\n",
        "\n",
        "# 4. Calculate the Recall score for the fraud class (label 1)\n",
        "rf_smote_true_positives = rf_smote_pred.filter((rf_smote_pred.TX_FRAUD == 1) & (rf_smote_pred.prediction == 1)).count()\n",
        "rf_smote_false_negatives = rf_smote_pred.filter((rf_smote_pred.TX_FRAUD == 1) & (rf_smote_pred.prediction == 0)).count()\n",
        "\n",
        "rf_smote_recall_label_1 = rf_smote_true_positives / (rf_smote_true_positives + rf_smote_false_negatives) if (rf_smote_true_positives + rf_smote_false_negatives) > 0 else 0\n",
        "\n",
        "# 5. Print the calculated Recall score\n",
        "print(f\"Random Forest Recall for Class 1 (Fraud) (After SMOTE): {rf_smote_recall_label_1}\")\n",
        "\n",
        "# Show confusion matrix for RF model after SMOTE\n",
        "rf_smote_pred.groupBy(\"TX_FRAUD\", \"prediction\").count().orderBy(\"TX_FRAUD\", \"prediction\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18300662"
      },
      "source": [
        "## Summary and Comparison\n",
        "\n",
        "### Subtask:\n",
        "Present a comprehensive summary of all AUC and Recall scores, comparing the performance of both models before and after SMOTE. Discuss the findings.\n",
        "\n",
        "### Instructions:\n",
        "1. Create a summary table or dictionary of all calculated AUC and Recall scores for:\n",
        "    *   Logistic Regression (Before SMOTE)\n",
        "    *   Random Forest (Before SMOTE)\n",
        "    *   Logistic Regression (After SMOTE)\n",
        "    *   Random Forest (After SMOTE)\n",
        "2. Print this summary.\n",
        "3. Discuss the findings based on the comparison of these metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa23df3d"
      },
      "source": [
        "**Reasoning**:\n",
        "All models have been trained and evaluated, both before and after applying SMOTE. The next step is to create a summary of all calculated AUC and Recall scores and then provide a discussion of the findings, as specified in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a48e13a3",
        "outputId": "a80f82f0-ca9d-4993-cdea-35a71f2f0a99"
      },
      "outputs": [],
      "source": [
        "print(\"### Summary of Model Performance\\n\")\n",
        "\n",
        "results = {\n",
        "    \"Logistic Regression (Before SMOTE)\": {\n",
        "        \"AUC\": auc,\n",
        "        \"Recall\": recall_label_1\n",
        "    },\n",
        "    \"Random Forest (Before SMOTE)\": {\n",
        "        \"AUC\": rf_auc,\n",
        "        \"Recall\": rf_recall_label_1\n",
        "    },\n",
        "    \"Logistic Regression (After SMOTE)\": {\n",
        "        \"AUC\": lr_smote_auc,\n",
        "        \"Recall\": lr_smote_recall_label_1\n",
        "    },\n",
        "    \"Random Forest (After SMOTE)\": {\n",
        "        \"AUC\": rf_smote_auc,\n",
        "        \"Recall\": rf_smote_recall_label_1\n",
        "    }\n",
        "}\n",
        "\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"{model_name}:\")\n",
        "    print(f\"  AUC: {metrics['AUC']:.4f}\")\n",
        "    print(f\"  Recall (Fraud Class): {metrics['Recall']:.4f}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "88eaff42",
        "outputId": "e48bded4-3ac1-4471-8846-40430ff558e1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import pandas as pd\n",
        "\n",
        "# Function to get probabilities and true labels from Spark DataFrame\n",
        "def get_roc_data(spark_df, label_col=\"TX_FRAUD\", prob_col=\"probability\"):\n",
        "    pdf = spark_df.select(label_col, prob_col).toPandas()\n",
        "    y_true = pdf[label_col].values\n",
        "    # Extract probability of the positive class (label 1)\n",
        "    y_proba = pdf[prob_col].apply(lambda x: x[1]).values\n",
        "    return y_true, y_proba\n",
        "\n",
        "# Prepare data for plotting ROC curves\n",
        "lr_y_true, lr_y_proba = get_roc_data(lr_pred)\n",
        "rf_y_true, rf_y_proba = get_roc_data(rf_pred)\n",
        "lr_smote_y_true, lr_smote_y_proba = get_roc_data(lr_smote_pred)\n",
        "rf_smote_y_true, rf_smote_y_proba = get_roc_data(rf_smote_pred)\n",
        "\n",
        "# Calculate ROC curve and AUC for each model\n",
        "lr_fpr, lr_tpr, _ = roc_curve(lr_y_true, lr_y_proba)\n",
        "lr_auc = auc(lr_fpr, lr_tpr)\n",
        "\n",
        "rf_fpr, rf_tpr, _ = roc_curve(rf_y_true, rf_y_proba)\n",
        "rf_auc_val = auc(rf_fpr, rf_tpr) # Renamed to avoid conflict with existing rf_auc variable\n",
        "\n",
        "lr_smote_fpr, lr_smote_tpr, _ = roc_curve(lr_smote_y_true, lr_smote_y_proba)\n",
        "lr_smote_auc_val = auc(lr_smote_fpr, lr_smote_tpr)\n",
        "\n",
        "rf_smote_fpr, rf_smote_tpr, _ = roc_curve(rf_smote_y_true, rf_smote_y_proba)\n",
        "rf_smote_auc_val = auc(rf_smote_fpr, rf_smote_tpr)\n",
        "\n",
        "# Plotting the ROC curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(lr_fpr, lr_tpr, color='blue', lw=2, label=f'LR (Before SMOTE) AUC = {lr_auc:.2f}')\n",
        "plt.plot(rf_fpr, rf_tpr, color='green', lw=2, label=f'RF (Before SMOTE) AUC = {rf_auc_val:.2f}')\n",
        "plt.plot(lr_smote_fpr, lr_smote_tpr, color='red', lw=2, label=f'LR (After SMOTE) AUC = {lr_smote_auc_val:.2f}')\n",
        "plt.plot(rf_smote_fpr, rf_smote_tpr, color='purple', lw=2, label=f'RF (After SMOTE) AUC = {rf_smote_auc_val:.2f}')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve Comparison')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18904169"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Logistic Regression (Before SMOTE)**: Achieved an AUC of 0.9914 and a Recall for the fraud class of 0.9758.\n",
        "*   **Random Forest (Before SMOTE)**: Achieved an AUC of 0.9888 and a Recall for the fraud class of 0.9426.\n",
        "*   **Impact of SMOTE on Training Data**: The SMOTE algorithm successfully balanced the training dataset, resulting in an equal number of instances (168,763) for both fraud and non-fraud classes.\n",
        "*   **Logistic Regression (After SMOTE)**:\n",
        "    *   The AUC remained high at 0.9914, showing consistent overall discrimination.\n",
        "    *   The Recall for the fraud class improved to 0.9835, indicating better detection of fraudulent transactions after balancing the training data.\n",
        "*   **Random Forest (After SMOTE)**:\n",
        "    *   The AUC increased slightly to 0.9914, demonstrating improved overall performance.\n",
        "    *   The Recall for the fraud class significantly improved to 0.9750, addressing its lower baseline recall.\n",
        "*   **Post-SMOTE Model Comparison**: After applying SMOTE, both Logistic Regression and Random Forest models demonstrated excellent and very similar AUC scores (0.9914). Logistic Regression maintained a slight edge in Recall for the fraud class (0.9835) compared to Random Forest (0.9750).\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **SMOTE's Effectiveness**: Applying SMOTE significantly improved the recall for the minority fraud class in both Logistic Regression (from 0.9758 to 0.9835) and Random Forest (from 0.9426 to 0.9750) models, making it a valuable strategy for handling imbalanced datasets in fraud detection.\n",
        "*   **Model Recommendation**: Given the critical importance of minimizing false negatives in fraud detection, the Logistic Regression model trained with SMOTE-balanced data (Recall: 0.9835) appears to be the marginally preferred choice over Random Forest with SMOTE (Recall: 0.9750) in this scenario.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyN1uYckrlzxlYX+6sqqKDeO",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
