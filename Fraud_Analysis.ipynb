{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb00b8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gdown\n",
    "# %pip install pyspark\n",
    "# %pip install matplotlib\n",
    "# %pip install numpy\n",
    "# %pip install pandas\n",
    "# %pip install seaborn\n",
    "# %pip install scikit-learn\n",
    "# %pip install scipy\n",
    "# %pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8d7d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Fraud Detection Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9bc99c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- TRANSACTION_ID: integer (nullable = true)\n",
      " |-- TX_DATETIME: timestamp (nullable = true)\n",
      " |-- CUSTOMER_ID: integer (nullable = true)\n",
      " |-- TERMINAL_ID: integer (nullable = true)\n",
      " |-- TX_AMOUNT: double (nullable = true)\n",
      " |-- TX_TIME_SECONDS: integer (nullable = true)\n",
      " |-- TX_TIME_DAYS: integer (nullable = true)\n",
      " |-- TX_FRAUD: integer (nullable = true)\n",
      " |-- TX_FRAUD_SCENARIO: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "file_name = \"Final Transaction.csv\"\n",
    "\n",
    "if not os.path.exists(file_name):\n",
    "    url = \"https://drive.google.com/file/d/1qJtOG4ReSYyaHn0xbitvXqESQS94OkqP/view?usp=drive_link\"\n",
    "    gdown.download(url, file_name, quiet=True, fuzzy=True)\n",
    "\n",
    "df = spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87b18799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|_c0|TRANSACTION_ID|        TX_DATETIME|CUSTOMER_ID|TERMINAL_ID|TX_AMOUNT|TX_TIME_SECONDS|TX_TIME_DAYS|TX_FRAUD|TX_FRAUD_SCENARIO|\n",
      "+---+--------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|  0|             0|2023-01-01 00:00:31|        596|       3156|   533.07|             31|           0|       0|                0|\n",
      "|  1|             1|2023-01-01 00:02:10|       4961|       3412|   808.56|            130|           0|       0|                0|\n",
      "|  2|             2|2023-01-01 00:07:56|          2|       1365|  1442.94|            476|           0|       1|                1|\n",
      "|  3|             3|2023-01-01 00:09:29|       4128|       8737|   620.65|            569|           0|       0|                0|\n",
      "|  4|             4|2023-01-01 00:10:34|        927|       9906|   490.66|            634|           0|       0|                0|\n",
      "|  5|             5|2023-01-01 00:10:45|        568|       8803|   401.17|            645|           0|       0|                0|\n",
      "|  6|             6|2023-01-01 00:11:30|       2803|       5490|   938.54|            690|           0|       0|                0|\n",
      "|  7|             7|2023-01-01 00:11:44|       4684|       2486|   206.53|            704|           0|       0|                0|\n",
      "|  8|             8|2023-01-01 00:11:53|       4128|       8354|   253.47|            713|           0|       0|                0|\n",
      "|  9|             9|2023-01-01 00:13:44|        541|       6212|   555.63|            824|           0|       0|                0|\n",
      "+---+--------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/01 13:47:50 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , TRANSACTION_ID, TX_DATETIME, CUSTOMER_ID, TERMINAL_ID, TX_AMOUNT, TX_TIME_SECONDS, TX_TIME_DAYS, TX_FRAUD, TX_FRAUD_SCENARIO\n",
      " Schema: _c0, TRANSACTION_ID, TX_DATETIME, CUSTOMER_ID, TERMINAL_ID, TX_AMOUNT, TX_TIME_SECONDS, TX_TIME_DAYS, TX_FRAUD, TX_FRAUD_SCENARIO\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/bl-sw/code/Fraud-Detection-Kaggle/Final%20Transaction.csv\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51db45c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+------------+--------+\n",
      "|        TX_DATETIME|TX_AMOUNT|TX_TIME_SECONDS|TX_TIME_DAYS|TX_FRAUD|\n",
      "+-------------------+---------+---------------+------------+--------+\n",
      "|2023-01-01 00:00:31|   533.07|             31|           0|       0|\n",
      "|2023-01-01 00:02:10|   808.56|            130|           0|       0|\n",
      "|2023-01-01 00:07:56|  1442.94|            476|           0|       1|\n",
      "|2023-01-01 00:09:29|   620.65|            569|           0|       0|\n",
      "|2023-01-01 00:10:34|   490.66|            634|           0|       0|\n",
      "+-------------------+---------+---------------+------------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "remove_cols = ['_c0', 'TRANSACTION_ID', 'CUSTOMER_ID', 'TERMINAL_ID', 'TX_FRAUD_SCENARIO']\n",
    "df = df.drop(*remove_cols)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc6558",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "## 1. Dataset Overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38453c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[summary: string, TX_AMOUNT: string, TX_TIME_SECONDS: string, TX_TIME_DAYS: string, TX_FRAUD: string]\n",
      "root\n",
      " |-- TX_DATETIME: timestamp (nullable = true)\n",
      " |-- TX_AMOUNT: double (nullable = true)\n",
      " |-- TX_TIME_SECONDS: integer (nullable = true)\n",
      " |-- TX_TIME_DAYS: integer (nullable = true)\n",
      " |-- TX_FRAUD: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.describe())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3fe650d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['TX_AMOUNT']\n",
    "categorical_cols = ['']\n",
    "target_cols = ['TX_FRAUD']\n",
    "timestamp_cols = ['TX_DATETIME']\n",
    "relative_time_cols = [\n",
    "    \"TX_TIME_SECONDS\",\n",
    "    \"TX_TIME_DAYS\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c4e963",
   "metadata": {},
   "source": [
    "## 2. Missing Values Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68bd0f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------------+------------+--------+\n",
      "|TX_DATETIME|TX_AMOUNT|TX_TIME_SECONDS|TX_TIME_DAYS|TX_FRAUD|\n",
      "+-----------+---------+---------------+------------+--------+\n",
      "|0          |0        |0              |0           |0       |\n",
      "+-----------+---------+---------------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnan, count, when\n",
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "exprs = []\n",
    "\n",
    "for field in df.schema.fields:\n",
    "    c = field.name\n",
    "    dtype = field.dataType\n",
    "\n",
    "    # Numeric → check NULL + NaN\n",
    "    if isinstance(dtype, (DoubleType, FloatType)):\n",
    "        exprs.append(\n",
    "            count(\n",
    "                when(col(c).isNull() | isnan(col(c)), c)\n",
    "            ).alias(c)\n",
    "        )\n",
    "    # Non-numeric → chỉ check NULL\n",
    "    else:\n",
    "        exprs.append(\n",
    "            count(\n",
    "                when(col(c).isNull(), c)\n",
    "            ).alias(c)\n",
    "        )\n",
    "\n",
    "missing_df = df.select(exprs)\n",
    "missing_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b984fb",
   "metadata": {},
   "source": [
    "## 3. Descriptive Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65812a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+-----------------+-----------------+\n",
      "|summary|         TX_AMOUNT|           TX_FRAUD|  TX_TIME_SECONDS|     TX_TIME_DAYS|\n",
      "+-------+------------------+-------------------+-----------------+-----------------+\n",
      "|  count|           1754155|            1754155|          1754155|          1754155|\n",
      "|   mean|  539.681997280744| 0.1345200395632085|7903233.708571933|90.97260276315377|\n",
      "| stddev|1179.7105939984608|0.34121029423187604|4565172.383899659|52.83709109961424|\n",
      "|    min|               0.0|                  0|               31|                0|\n",
      "|    max|          647837.5|                  1|         15811197|              182|\n",
      "+-------+------------------+-------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    *numerical_cols,\n",
    "    *target_cols,\n",
    "    *timestamp_cols,\n",
    "    *relative_time_cols\n",
    ").describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0619e0c",
   "metadata": {},
   "source": [
    "## 4. Target Variable Distribution (TX_FRAUD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5b24622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----------------+\n",
      "|TX_FRAUD|count  |ratio            |\n",
      "+--------+-------+-----------------+\n",
      "|0       |1518186|86.54799604367915|\n",
      "|1       |235969 |13.45200395632085|\n",
      "+--------+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total = df.count()\n",
    "\n",
    "fraud_dist = (\n",
    "    df.groupBy(\"TX_FRAUD\")\n",
    "      .count()\n",
    "      .withColumn(\"ratio\", col(\"count\") / total * 100)\n",
    "      .orderBy(\"TX_FRAUD\")\n",
    ")\n",
    "\n",
    "fraud_dist.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc1da9",
   "metadata": {},
   "source": [
    "## 5. Important Features Analysis\n",
    "\n",
    "### 5.1. TX_AMOUNT Analysis (Transaction Amount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa35f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TX_AMOUNT ANALYSIS BY TX_FRAUD\n",
      "============================================================\n",
      "+--------+------------------+------------------+----------+----------+-------+\n",
      "|TX_FRAUD|mean_amount       |stddev_amount     |min_amount|max_amount|count  |\n",
      "+--------+------------------+------------------+----------+----------+-------+\n",
      "|0       |393.90920306207437|275.34467418810556|0.0       |1000.0    |1518186|\n",
      "|1       |1477.5603430111585|2973.50249720581  |0.2       |647837.5  |235969 |\n",
      "+--------+------------------+------------------+----------+----------+-------+\n",
      "\n",
      "\n",
      "Detailed statistics:\n",
      "\n",
      "Non-Fraud:\n",
      "  Mean:    393.91\n",
      "  StdDev:  275.34\n",
      "  Min:     0.00\n",
      "  Max:     1000.00\n",
      "  Count:   1,518,186\n",
      "\n",
      "Fraud:\n",
      "  Mean:    1477.56\n",
      "  StdDev:  2973.50\n",
      "  Min:     0.20\n",
      "  Max:     647837.50\n",
      "  Count:   235,969\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, mean, stddev, min as spark_min, max as spark_max, count\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TX_AMOUNT ANALYSIS BY TX_FRAUD\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "amount_stats = (\n",
    "    df.groupBy(\"TX_FRAUD\")\n",
    "      .agg(\n",
    "          mean(\"TX_AMOUNT\").alias(\"mean_amount\"),\n",
    "          stddev(\"TX_AMOUNT\").alias(\"stddev_amount\"),\n",
    "          spark_min(\"TX_AMOUNT\").alias(\"min_amount\"),\n",
    "          spark_max(\"TX_AMOUNT\").alias(\"max_amount\"),\n",
    "          count(\"*\").alias(\"count\")\n",
    "      )\n",
    "      .orderBy(\"TX_FRAUD\")\n",
    ")\n",
    "\n",
    "\n",
    "amount_stats.show(truncate=False)\n",
    "\n",
    "# Display detailed statistics\n",
    "print(\"\\nDetailed statistics:\")\n",
    "for row in amount_stats.collect():\n",
    "    fraud_label = \"Fraud\" if row['TX_FRAUD'] == 1 else \"Non-Fraud\"\n",
    "    print(f\"\\n{fraud_label}:\")\n",
    "    print(f\"  Mean:    {row['mean_amount']:.2f}\")\n",
    "    print(f\"  StdDev:  {row['stddev_amount']:.2f}\")\n",
    "    print(f\"  Min:     {row['min_amount']:.2f}\")\n",
    "    print(f\"  Max:     {row['max_amount']:.2f}\")\n",
    "    print(f\"  Count:   {row['count']:,}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "for label, name in [(0, \"Non-Fraud\"), (1, \"Fraud\")]:\n",
    "    subset = sample_df[sample_df[\"TX_FRAUD\"] == label][\"TX_AMOUNT\"]\n",
    "    plt.hist(\n",
    "        np.log1p(subset),\n",
    "        bins=50,\n",
    "        alpha=0.6,\n",
    "        label=name\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"log(1 + TX_AMOUNT)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"TX_AMOUNT Distribution (Log Scale)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f010af",
   "metadata": {},
   "source": [
    "### 5.2. Analysis of Unique Customers and Terminals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9466b974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANALYSIS OF UNIQUE CUSTOMERS AND TERMINALS\n",
      "============================================================\n",
      "\n",
      "Number of unique customers: 4,990\n",
      "Number of unique terminals: 10,000\n",
      "Average number of transactions per customer: 351.53\n",
      "Average number of transactions per terminal: 175.42\n",
      "\n",
      "Top 10 customers with most transactions:\n",
      "+-----------+-----+\n",
      "|CUSTOMER_ID|count|\n",
      "+-----------+-----+\n",
      "|        382|  767|\n",
      "|       3864|  762|\n",
      "|       2891|  761|\n",
      "|        775|  754|\n",
      "|       3651|  752|\n",
      "|       1411|  752|\n",
      "|        149|  749|\n",
      "|       2932|  747|\n",
      "|        732|  746|\n",
      "|        379|  743|\n",
      "+-----------+-----+\n",
      "\n",
      "\n",
      "Top 10 terminals with most transactions:\n",
      "+-----------+-----+\n",
      "|TERMINAL_ID|count|\n",
      "+-----------+-----+\n",
      "|       4018|  376|\n",
      "|        692|  372|\n",
      "|       5295|  368|\n",
      "|       8130|  360|\n",
      "|        872|  356|\n",
      "|       8670|  354|\n",
      "|       7884|  346|\n",
      "|       8606|  338|\n",
      "|       2475|  334|\n",
      "|       7798|  333|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, col\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ANALYSIS OF UNIQUE CUSTOMERS AND TERMINALS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Recalculate total_rows to ensure we have the value\n",
    "total_rows = df.count()\n",
    "\n",
    "# Number of unique customers and terminals\n",
    "unique_customers = df.select(countDistinct(\"CUSTOMER_ID\")).collect()[0][0]\n",
    "unique_terminals = df.select(countDistinct(\"TERMINAL_ID\")).collect()[0][0]\n",
    "\n",
    "print(f\"\\nNumber of unique customers: {unique_customers:,}\")\n",
    "print(f\"Number of unique terminals: {unique_terminals:,}\")\n",
    "\n",
    "# Average number of transactions per customer\n",
    "avg_tx_per_customer = total_rows / unique_customers if unique_customers > 0 else 0\n",
    "print(f\"Average number of transactions per customer: {avg_tx_per_customer:.2f}\")\n",
    "\n",
    "# Average number of transactions per terminal\n",
    "avg_tx_per_terminal = total_rows / unique_terminals if unique_terminals > 0 else 0\n",
    "print(f\"Average number of transactions per terminal: {avg_tx_per_terminal:.2f}\")\n",
    "\n",
    "# Top 10 customers with most transactions\n",
    "print(\"\\nTop 10 customers with most transactions:\")\n",
    "top_customers = df.groupBy(\"CUSTOMER_ID\").count().orderBy(col(\"count\").desc()).limit(10)\n",
    "top_customers.show()\n",
    "\n",
    "# Top 10 terminals with most transactions\n",
    "print(\"\\nTop 10 terminals with most transactions:\")\n",
    "top_terminals = df.groupBy(\"TERMINAL_ID\").count().orderBy(col(\"count\").desc()).limit(10)\n",
    "top_terminals.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c26388f",
   "metadata": {},
   "source": [
    "### 5.3. Time-based Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32831617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TIME-BASED ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Time range:\n",
      "  Start date: 2023-01-01\n",
      "  End date: 2023-07-02\n",
      "\n",
      "Top 10 days with most transactions:\n",
      "+----------+--------+--------+\n",
      "|TX_DATE   |total_tx|fraud_tx|\n",
      "+----------+--------+--------+\n",
      "|2023-04-01|9789    |1295    |\n",
      "|2023-04-27|9787    |1344    |\n",
      "|2023-04-06|9784    |1313    |\n",
      "|2023-02-02|9771    |1414    |\n",
      "|2023-02-15|9767    |1319    |\n",
      "|2023-06-11|9762    |1367    |\n",
      "|2023-01-30|9760    |1244    |\n",
      "|2023-01-13|9753    |1349    |\n",
      "|2023-02-09|9749    |1312    |\n",
      "|2023-01-03|9747    |1309    |\n",
      "+----------+--------+--------+\n",
      "\n",
      "\n",
      "Transaction distribution by hour of day:\n",
      "+-------+--------+--------+\n",
      "|TX_HOUR|total_tx|fraud_tx|\n",
      "+-------+--------+--------+\n",
      "|0      |15137   |2031    |\n",
      "|1      |21861   |2968    |\n",
      "|2      |30140   |4051    |\n",
      "|3      |40380   |5306    |\n",
      "|4      |52403   |6887    |\n",
      "|5      |65168   |8811    |\n",
      "|6      |79880   |10925   |\n",
      "|7      |94030   |12795   |\n",
      "|8      |106617  |14411   |\n",
      "|9      |116864  |15573   |\n",
      "|10     |125804  |16793   |\n",
      "|11     |128909  |17556   |\n",
      "|12     |129108  |17371   |\n",
      "|13     |125516  |16876   |\n",
      "|14     |116965  |15859   |\n",
      "|15     |106584  |14178   |\n",
      "|16     |93251   |12431   |\n",
      "|17     |79551   |10608   |\n",
      "|18     |66043   |9011    |\n",
      "|19     |52204   |6940    |\n",
      "|20     |40557   |5506    |\n",
      "|21     |30064   |4095    |\n",
      "|22     |21873   |2915    |\n",
      "|23     |15246   |2072    |\n",
      "+-------+--------+--------+\n",
      "\n",
      "\n",
      "Transaction distribution by day of week:\n",
      "Day        | Total Transactions | Fraud Transactions\n",
      "--------------------------------------------------\n",
      "Sunday     |         259,501 |          34,741\n",
      "Monday     |         249,707 |          33,100\n",
      "Tuesday    |         248,971 |          33,683\n",
      "Wednesday  |         248,716 |          33,640\n",
      "Thursday   |         249,579 |          33,457\n",
      "Friday     |         248,797 |          33,811\n",
      "Saturday   |         248,884 |          33,537\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, hour, dayofweek, count as spark_count, col, when, min as spark_min, max as spark_max\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TIME-BASED ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert TX_DATETIME to date and extract time features\n",
    "df_time = df.withColumn(\"TX_DATE\", to_date(\"TX_DATETIME\")) \\\n",
    "            .withColumn(\"TX_HOUR\", hour(\"TX_DATETIME\")) \\\n",
    "            .withColumn(\"TX_DAYOFWEEK\", dayofweek(\"TX_DATETIME\"))\n",
    "\n",
    "# Time range of the dataset\n",
    "date_range = df_time.select(\n",
    "    spark_min(\"TX_DATE\").alias(\"min_date\"),\n",
    "    spark_max(\"TX_DATE\").alias(\"max_date\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nTime range:\")\n",
    "print(f\"  Start date: {date_range['min_date']}\")\n",
    "print(f\"  End date: {date_range['max_date']}\")\n",
    "\n",
    "# Transaction distribution by day\n",
    "print(\"\\nTop 10 days with most transactions:\")\n",
    "daily_tx = df_time.groupBy(\"TX_DATE\").agg(\n",
    "    spark_count(\"*\").alias(\"total_tx\"),\n",
    "    spark_count(when(col(\"TX_FRAUD\") == 1, 1)).alias(\"fraud_tx\")\n",
    ").orderBy(col(\"total_tx\").desc()).limit(10)\n",
    "daily_tx.show(truncate=False)\n",
    "\n",
    "# Transaction distribution by hour\n",
    "print(\"\\nTransaction distribution by hour of day:\")\n",
    "hourly_tx = df_time.groupBy(\"TX_HOUR\").agg(\n",
    "    spark_count(\"*\").alias(\"total_tx\"),\n",
    "    spark_count(when(col(\"TX_FRAUD\") == 1, 1)).alias(\"fraud_tx\")\n",
    ").orderBy(\"TX_HOUR\")\n",
    "hourly_tx.show(24, truncate=False)\n",
    "\n",
    "# Transaction distribution by day of week\n",
    "print(\"\\nTransaction distribution by day of week:\")\n",
    "# 1 = Sunday, 2 = Monday, ..., 7 = Saturday\n",
    "day_names = {1: \"Sunday\", 2: \"Monday\", 3: \"Tuesday\", 4: \"Wednesday\", \n",
    "             5: \"Thursday\", 6: \"Friday\", 7: \"Saturday\"}\n",
    "dow_tx = df_time.groupBy(\"TX_DAYOFWEEK\").agg(\n",
    "    spark_count(\"*\").alias(\"total_tx\"),\n",
    "    spark_count(when(col(\"TX_FRAUD\") == 1, 1)).alias(\"fraud_tx\")\n",
    ").orderBy(\"TX_DAYOFWEEK\")\n",
    "\n",
    "print(\"Day        | Total Transactions | Fraud Transactions\")\n",
    "print(\"-\" * 50)\n",
    "for row in dow_tx.collect():\n",
    "    day_name = day_names.get(row['TX_DAYOFWEEK'], f\"Day {row['TX_DAYOFWEEK']}\")\n",
    "    print(f\"{day_name:10s} | {row['total_tx']:>15,} | {row['fraud_tx']:>15,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec6cdaf",
   "metadata": {},
   "source": [
    "### 5.4. TX_FRAUD_SCENARIO Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ec941cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TX_FRAUD_SCENARIO ANALYSIS\n",
      "============================================================\n",
      "+-----------------+-------+\n",
      "|TX_FRAUD_SCENARIO|  count|\n",
      "+-----------------+-------+\n",
      "|                0|1518186|\n",
      "|                1| 222261|\n",
      "|                2|   9077|\n",
      "|                3|   4631|\n",
      "+-----------------+-------+\n",
      "\n",
      "\n",
      "Fraud scenario details:\n",
      "  Non-Fraud           :    1,518,186 ( 86.55%)\n",
      "  Fraud Scenario 1    :      222,261 ( 12.67%)\n",
      "  Fraud Scenario 2    :        9,077 (  0.52%)\n",
      "  Fraud Scenario 3    :        4,631 (  0.26%)\n",
      "\n",
      "TX_AMOUNT statistics by scenario:\n",
      "+-----------------+------------------+------------------+----------+----------+\n",
      "|TX_FRAUD_SCENARIO|mean_amount       |stddev_amount     |min_amount|max_amount|\n",
      "+-----------------+------------------+------------------+----------+----------+\n",
      "|0                |393.90920306207437|275.34467418810556|0.0       |1000.0    |\n",
      "|1                |1281.7617225694128|242.2168450569149 |1000.01   |3094.88   |\n",
      "|2                |515.0349080092544 |403.09473553799927|0.2       |2346.56   |\n",
      "|3                |12761.352299719283|17789.750444146342|5.0       |647837.5  |\n",
      "+-----------------+------------------+------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count as spark_count, mean, stddev, min as spark_min, max as spark_max\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TX_FRAUD_SCENARIO ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Recalculate total_rows to ensure we have the value\n",
    "total_rows = df.count()\n",
    "\n",
    "# Distribution of fraud scenario types\n",
    "scenario_dist = df.groupBy(\"TX_FRAUD_SCENARIO\").agg(\n",
    "    spark_count(\"*\").alias(\"count\")\n",
    ").orderBy(\"TX_FRAUD_SCENARIO\")\n",
    "\n",
    "scenario_dist.show()\n",
    "\n",
    "# Scenario details\n",
    "print(\"\\nFraud scenario details:\")\n",
    "for row in scenario_dist.collect():\n",
    "    scenario = row['TX_FRAUD_SCENARIO']\n",
    "    count = row['count']\n",
    "    percentage = (count / total_rows) * 100\n",
    "    scenario_name = \"Non-Fraud\" if scenario == 0 else f\"Fraud Scenario {scenario}\"\n",
    "    print(f\"  {scenario_name:20s}: {count:>12,} ({percentage:>6.2f}%)\")\n",
    "\n",
    "# Scenario analysis by TX_AMOUNT\n",
    "print(\"\\nTX_AMOUNT statistics by scenario:\")\n",
    "scenario_amount_stats = df.groupBy(\"TX_FRAUD_SCENARIO\").agg(\n",
    "    mean(\"TX_AMOUNT\").alias(\"mean_amount\"),\n",
    "    stddev(\"TX_AMOUNT\").alias(\"stddev_amount\"),\n",
    "    spark_min(\"TX_AMOUNT\").alias(\"min_amount\"),\n",
    "    spark_max(\"TX_AMOUNT\").alias(\"max_amount\")\n",
    ").orderBy(\"TX_FRAUD_SCENARIO\")\n",
    "\n",
    "scenario_amount_stats.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7358b546",
   "metadata": {},
   "source": [
    "## 6. Summary and Insights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66c2947c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SUMMARY AND INSIGHTS\n",
      "============================================================\n",
      "\n",
      "1. DATASET STRUCTURE:\n",
      "   - Total number of transactions: 1,754,155\n",
      "   - Number of features: 8\n",
      "   - Number of unique customers: 4,990\n",
      "   - Number of unique terminals: 10,000\n",
      "\n",
      "2. TARGET DISTRIBUTION:\n",
      "   - Fraud transactions: 235,969 (13.45%)\n",
      "   - Non-Fraud transactions: 1,518,186 (86.55%)\n",
      "   - Class imbalance ratio: 6.43:1\n",
      "\n",
      "3. KEY CHARACTERISTICS:\n",
      "   - Average transaction amount: 539.68\n",
      "   - Average amount (Fraud): 1477.56\n",
      "   - Average amount (Non-Fraud): 393.91\n",
      "\n",
      "4. RECOMMENDATIONS:\n",
      "   - Dataset has severe class imbalance, needs to be handled when training model\n",
      "   - Need deeper analysis on relationship between TX_AMOUNT and TX_FRAUD\n",
      "   - Should create additional features from TX_DATETIME (hour, day of week, etc.)\n",
      "   - Can create aggregate features for each CUSTOMER_ID and TERMINAL_ID\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, mean, countDistinct\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY AND INSIGHTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Recalculate values to ensure we have data\n",
    "total_rows = df.count()\n",
    "total_cols = len(df.columns)\n",
    "unique_customers = df.select(countDistinct(\"CUSTOMER_ID\")).collect()[0][0]\n",
    "unique_terminals = df.select(countDistinct(\"TERMINAL_ID\")).collect()[0][0]\n",
    "\n",
    "print(\"\\n1. DATASET STRUCTURE:\")\n",
    "print(f\"   - Total number of transactions: {total_rows:,}\")\n",
    "print(f\"   - Number of features: {total_cols}\")\n",
    "print(f\"   - Number of unique customers: {unique_customers:,}\")\n",
    "print(f\"   - Number of unique terminals: {unique_terminals:,}\")\n",
    "\n",
    "print(\"\\n2. TARGET DISTRIBUTION:\")\n",
    "fraud_info = df.filter(col(\"TX_FRAUD\") == 1).count()\n",
    "non_fraud_info = df.filter(col(\"TX_FRAUD\") == 0).count()\n",
    "fraud_pct = (fraud_info / total_rows) * 100\n",
    "print(f\"   - Fraud transactions: {fraud_info:,} ({fraud_pct:.2f}%)\")\n",
    "print(f\"   - Non-Fraud transactions: {non_fraud_info:,} ({100-fraud_pct:.2f}%)\")\n",
    "if fraud_info > 0:\n",
    "    print(f\"   - Class imbalance ratio: {(non_fraud_info/fraud_info):.2f}:1\")\n",
    "\n",
    "print(\"\\n3. KEY CHARACTERISTICS:\")\n",
    "# Calculate some important metrics\n",
    "avg_amount = df.select(mean(\"TX_AMOUNT\")).collect()[0][0]\n",
    "avg_amount_fraud = df.filter(col(\"TX_FRAUD\") == 1).select(mean(\"TX_AMOUNT\")).collect()[0][0]\n",
    "avg_amount_non_fraud = df.filter(col(\"TX_FRAUD\") == 0).select(mean(\"TX_AMOUNT\")).collect()[0][0]\n",
    "\n",
    "print(f\"   - Average transaction amount: {avg_amount:.2f}\")\n",
    "if avg_amount_fraud:\n",
    "    print(f\"   - Average amount (Fraud): {avg_amount_fraud:.2f}\")\n",
    "if avg_amount_non_fraud:\n",
    "    print(f\"   - Average amount (Non-Fraud): {avg_amount_non_fraud:.2f}\")\n",
    "\n",
    "print(\"\\n4. RECOMMENDATIONS:\")\n",
    "print(\"   - Dataset has severe class imbalance, needs to be handled when training model\")\n",
    "print(\"   - Need deeper analysis on relationship between TX_AMOUNT and TX_FRAUD\")\n",
    "print(\"   - Should create additional features from TX_DATETIME (hour, day of week, etc.)\")\n",
    "print(\"   - Can create aggregate features for each CUSTOMER_ID and TERMINAL_ID\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b411b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
